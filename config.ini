target_column = text
_PARALLEL = True
_VERBOSE = True

[import_data]

    input_data_directories = datasets,
    output_data_directory  = data_import
    merge_columns = Title, Abstract

[phrase]

    f_abbreviations = abbreviations.csv
    output_data_directory = data_document_scores/

[parse]

    output_data_directory = data_parsed
    pipeline = dedash, titlecaps, replace_acronyms, separated_parenthesis, replace_from_dictionary, token_replacement, decaps_text, pos_tokenizer

    [[replace_from_dictionary]]
	prefix = 'MeSH_'
	
    [[replace_acronyms]]
	prefix = 'PHRASE'

   [[separated_parenthesis]]
        # Only keep long parenthetical content
	min_keep_length=10

    [[pos_tokenizer]]
        POS_blacklist = connector, cardinal, pronoun, symbol, punctuation, modal_verb, adverb, verb, w_word, adjective

[embed]

    input_data_directory  = data_parsed
    output_data_directory = data_embeddings
    
    embedding_commands    = w2v_embedding,

    [[w2v_embedding]]      
        f_db = w2v.gensim
      	skip_gram = 0
      	hierarchical_softmax = 1
        epoch_n = 30
        window = 5
	negative = 0
        sample = 1e-5
        size = 300
        min_count = 10

[score]

    output_data_directory = data_document_scores
    f_db  = document_scores.h5

    count_commands = term_document_frequency, term_frequency,
    score_commands = score_unique_IDF,
    compute_reduced_representation = False

    [[downsample_weights]]
        # Downsample weights, adjust as needed (zero value has no effect)
	aspect=1.0
	way=1.0
	implication=1.0
	understand=1.0
	relevance=1.0
	research=1.0
	example=1.0
	importance=1.0
	emphasi=1.0
	
	estimation=1.0
	accuracy=1.0
	estimate=1.0
	variable=1.0
	
	
    [[reduced_representation]]
        n_components = 25

    [[term_frequency]]
        f_db = TF.csv

    [[term_document_frequency]]   
        f_db = TDF.csv
	

[metacluster]

    score_method = unique_IDF

    subcluster_m = 5000
    #subcluster_kn = 26
    subcluster_kn = 25

    subcluster_pcut = 0.80
    subcluster_repeats = 1

    output_data_directory = data_clustering
    f_centroids = meta_cluster_centroids.h5

[postprocessing]

    compute_dispersion = False
    output_data_directory = results
    master_columns = 'Appl ID', 'Title', 'Fiscal Year'

    topn_words_returned = 10

    [[LIME_explainer]]
	metacluster_cosine_minsim = 0.6
	score_method = unique_IDF    
	n_lime_samples = 25 # Make this higher for more accuracy
	n_lime_features = 50
	n_estimators = 50
