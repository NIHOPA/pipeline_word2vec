#target_columns  = text,specificAims,
target_columns  = text,

_PARALLEL = True
#_PARALLEL = False
_FORCE = True

[import_data]

    input_data_directories = datasets,
    output_data_directory  = data_sql
    
    data_type = csv
    output_table = original

[phrase_identification]

    f_abbreviations = abbreviations.sqlite
    output_data_directory = data_document_scores
    output_table = abbreviations

[parse]

    output_table = parsed
    output_data_directory = data_parsed
    
    pipeline = dedash, titlecaps, replace_phrases, remove_parenthesis, replace_from_dictionary, token_replacement, decaps_text, pos_tokenizer

    [[replace_from_dictionary]]
        input_data_directory = w2v_pipeline/preprocessing/dictionaries
        f_dict = MeSH_two_word_lexicon.csv
    
    [[replace_phrases]]
        input_data_directory = data_document_scores
        f_abbreviations = abbreviations.sqlite

    [[pos_tokenizer]]
        POS_blacklist = connector, cardinal, pronoun, symbol, punctuation, modal_verb, adverb, verb, w_word


[score]

    output_data_directory = data_document_scores

    mapreduce_commands    = 
    globaldata_commands   = score_unique_TF, 

    [[negative_weights]]
        # Sample negative weights, adjust as needed
        variety = 0.5
        aim = 1.0
        specific = 1.0
        such = 1.0
        multitude = 0.5
        vital = 0.5
        development = 0.5
        important = 0.5
        plethora = 0.5
        target = 0.25
        scientific = 0.25
        collaborator = 1.0
        investigator = 1.0
        co-investigator = 1.0
        team = 1.0
        expert = 1.0
   
    [[term_frequency]]
        f_db = TF.sqlite
        command_whitelist =
    [[term_document_frequency]]   
        f_db = TF.sqlite

    [[score_simple]]
    [[score_unique]]
    [[score_simple_TF]]
    [[score_unique_TF]]
    [[score_locality_hash]]
        locality_n_bits = 12
        locality_alpha  = 0.00

    [[document_scores]]
        f_db  = document_scores.h5

[embedding]

    input_data_directory  = data_parsed
    output_data_directory = data_embeddings
    embedding_commands    = w2v_embedding,
    #embedding_commands    = w2v_embedding, d2v_embedding
       

    [[w2v_embedding]]      
        f_db = w2v.gensim
        epoch_n = 80
        window = 5
        negative = 5
        sample = 1e-5
        size = 300
        min_count = 10
    
[metacluster]

    score_method = unique_TF
    score_column = text

    subcluster_m = 3000
    subcluster_kn = 188
    
    subcluster_pcut = 0.80
    subcluster_repeats = 2

    output_data_directory = data_clustering
    #output_data_directory = data_clustering
    f_centroids = meta_cluster_centroids.h5